
EKS Cluster Process

1. Public - Private으로 구성된 Network망 구성, Public Private모두 외부 통신은 허용 되지만,
  외부로부터의 접근의 경우 Private Subnet은 접근금지
  - cloudformation에서 eks-network-infra1.1.yaml 실행

2. Network Test
 - Public과 Private에 각각 한대의 Instance를 생성한 후 Network Ping Test

3. Control Plane-Bastion Setting
 - Public에서 Bastion Host역할과 EKS Cluster의 Control Plane역할을 할 Server 설치 및 
  구성
 - $ aws --version => cli Version 확인
 - $ kubectl version --short --client => kubectl Version 확인
 - $ eksctl version => kubectl Version 확인

4. AWS configure(eksctl 명령어를 통하여 AWS Resource들을 생성하기 위해 필요합니다.)
 - $ aws configure => 설정
    AWS Access Key ID [None]:
    AWS Secret Access Key [None]:
    Default region name [None]:
    Default output format [None]:
   $ aws configure list => 설정확인

5. 현재 생성된 Nework 구성을 기준으로 EKS Cluster 생성
 - ekstest-cluster.yml File을 Bastion에서 Download
 - $ eksctl create cluster -f ekstest-cluster.yaml => ekscluster 생성
 *** eksctl을 사용하여 생성시 이점은 자동적으로 Autoscaling등에 필요한 Tag를 생성해 줍니다.
 - $ eskctl get cluster => cluster 생성 확인
 - $ kubectl get nodes -o wide => Amazon EKS Control plane에 등록된 Worker Node나열
 *** 만약 미리 생성된 Kubernetes Cluster에 연결을 한다면, 아래 명령어를 입력합니다.
    $ aws eks update-kubeconfig --region <your region> --name <your cluster name>
 *** Test-env삭제 Command
  - kubectl get pods => pod상태 조회
  - $ kubectl delete -f <file name> 
  - $ eksctl delete cluster --name <Cluster Name> => cluster를 삭제합니다.

6. Ingress Controller Setting
 *** Amazon EKS의 Application Load Balancing이란 Cluster에 Ingress 자원이 생성될 때 ALB 및 필요한
    자원이 생성되도록 Trigger하는 Controller입니다. Ingress자원들은 ALB를 구성하여 HTTP 또는 HTTPS 
    Trafic을 Cluster 내 Pod로 Routing합니다.
 *** Kubernetes의 Ingress의 경우, Application Load Balancers로 Provisioning됩니다.
 *** Kubernetes의 Service의 경우, Network Load Balancers로 Provisioning됩니다.
 *** AWS Load Balancer Controller에서 지원하는 Traffic Mode는 아래의 두 가지입니다.
    - Instance(default): Cluster 내 Node를 ALB의 대상으로 등록합니다. ALB에 도달하는 Traffic은 NodePort
                        로 Routing된 다음 Pod로 Porxy됩니다.
    - IP: Pod를 ALB대상으로 등록합니다. ALB에 도달하는 Traffic은 Pod로 직접 Routing됩니다. 해당 Traffic
         Mode를 사용하기 위해선 ingress.yaml File에 주석을 사용하여 명시적으로 지정해야 합니다.   
 - $ mkdir -p ~/manifest/alb-ingress-controller => 앞으로의 manifest를 관리하기 위해 manifest라는 
                                                   이름을 가진 Directory를 생성합니다. 
 - AWS Load Balancer Controller를 배포하기 전, Cluster에 대한 IAM OIDC(OpenID Connect) identity 
  Provider를 생성합니다. Kubernetes가 직접 관리하는 사용자 계정을 의미하는 Service Account에 IAM
  role을 사용하기 위해, 생성한 Cluster에 IAM OIDC Provider가 존재해야합니다.
   - $ eksctl utils associate-iam-oidc-provider \
      --region <Your region> \
      --cluster <cluster name> \
      --approve 
 - EKS Management Console에서 Cluster의 Details Tap의 OpenID Connect provider URL을 복사합니다.
  https://oidc.eks.ap-northeast-2.amazonaws.com/id/8D01635577E78B5BB9F3F164D34CA4B5
  위와 같은 형태의 URL에서 /id/ 뒷부분의 난수값을 아래의 명령어에 기입합니다.
   - $ aws iam list-open-id-connect-providers | grep DAA1FE5CE744EFB071BACB560C472C16
     => 해당 명령어를 입력하여 아래와 같은 값이 나타난다면 다음 단계를 진행합니다.
      "Arn": "arn:aws:iam::642057397966:oidc-provider/oidc.eks.ap-northeast-2.amazonaws.com/id/DAA1FE5CE744EFB071BACB560C472C16"
 - AWS LoadBalancer Controller에 부여할 IAM Policy생성
  - $ aws iam create-policy \
        --policy-name AWSLoadBalancerControllerIAMPolicy \
        --policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/install/iam_policy.json
 - AWS LoadBalancer Controller를 위한 Service Account를 생성
  - $ eksctl create iamserviceaccount \
        --cluster <My-Cluster> \ => 위에서 eksctl로 생성한 Cluster Name을 입력
        --namespace kube-system \
        --name aws-load-balnacer-controller \
        --attach-policy-arn arn:aws:iam::<Account_ID>:policy/AWSLoadBalancerControllerIAMPolicy \
        --override-existing-serviceaccounts \
        --approve
  *** EKS Cluster 배포시, AWS Load Balancer Controller와 관련된 IAM Policy를 Addon 형태로 Worker Node에
     추가하는 방법도 있습니다.
 - AWS Load Balancer Controller를 Cluster에 추가하는 작업을 수행합니다. 먼저, 인증서 구성을 Webhook에
  삽입할 수 있도록 cert-manager를 설치합니다. Cert-manager는 kubernetes Cluster내에서 TLS인증서를 자동적으로
  Provisioning 및 관리하는 Opensource입니다.
  - $ kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.yaml
 - LoadBalancer Controller yaml File을 download합니다.
  - $ cd ~/manifest/alb-load-balancer-controller
  - $ wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.1/v2_4_1_full.yaml
  - Download한 파일의 아래 --cluster-name부분을 편집합니다.
   spec:
    containers:
    - args:
        - --cluster-name=eks-demo # 생성한 클러스터 이름을 입력
        - --ingress-class=alb
        image: amazon/aws-alb-ingress-controller:v2.4.1
  - 그리고 동일한 yaml File에서 serviceaccount yaml spec을 없애줍니다. AWS Load Balancer Controller를
   위한 serviceaccount를 이미 생성했기 때문입니다. 아래의 내용을 삭제한 후, yaml File을 저장합니다.
   ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/name: aws-load-balancer-controller
      name: aws-load-balancer-controller
      namespace: kube-system
   ---
   - $ kubectl apply -f v2_4_1_full.yaml => Loadbalancer Controller apply
   *** 위의 명령어 입력시 Error가 발생한 경우
    - $ kubectl delete -f v2_4_1_full.yaml => 설치삭제
    - $ sudo yum -y install git => git 설치
    - $ kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master" git을 통한 parameter값 입력
    - $ kubectl apply -f v2_4_1_full.yaml => 재적용
   - $ kubectl get deployment -n kube-system aws-load-balancer-controller => Controller실행 확인 명령어
   - $ kubectl get sa aws-load-balancer-controller -n kube-system -o yaml => Service Account 적용확인
   - $ kubectl get po -n kube-system => Loadbalancer Addon pods 확인
    *** Cluster 내부에서 필요한 기능들을 위해 실행되는 Pod들을 Addon이라고 합니다. Addon에 사용되는 Pod들은
       Deployment, Replication Controller 등에 의해 관리됩니다. 그리고 이 Addon이 사용하는 Namespace가 
       kube-system입니다. Yaml File에서 Namespace를 kube-system으로 명시했기에 위의 명령어로 Pod 이름이 
       도출되면 정삭적으로 배포된것입니다.
   - $ kubectl logs -n kube-system $(kubectl get po -n kube-system | egrep -o "aws-load-balancer[a-zA-Z0-9-]+")
      => 관련 Log확인
   - $ ALBPOD=$(kubectl get pod -n kube-system | egrep -o "aws-load-balancer[a-zA-Z0-9-]+")
     $ kubectl describe pod -n kube-system ${ALBPOD} => 자세한 속성값 확인

7. Sample application을 위한 Container Image생성
  - $ sudo amazon-linux-extras install -y docker => Docker Engine 설치(Amazon Linux2용)
  - $ sudo service docker start => Docker Engine 시작
  - $ sudo usermod -a -G docker ec2-user => docker Command실행시 sudo를 사용하지 않고도 실행하기 위한 Group에 User 추가
  - 위의 명령어 적용을 위해 한 번 ec2-user 계정을 로그아웃 후 다시 접근 합니다.
  - $ docker info => sudo 제외 Command 확인
  - $ mkdir ~/sampleapp => sampleapp을 위한 작업 Directory를 생성합니다.
  - $ cd sampleapp => sampleapp Directory로 이동
  - $ git clone https://github.com/joozero/amazon-eks-flask.git => Containerize할 Source Code Download
  - $ aws ecr create-repository \       => Seoul Region에 demo-flask-bakcend라는 Repository생성 
      --repository-name demo-flask-backend \
      --image-scanning-configuration scanOnPush=true \
      --region ap-northeast-2
  - $ aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com
     => Contaner Image를 Repository에 Push하기 위해, 인증 토큰을 가지고 오고, 해당 인증 토큰을 docker login 명령어로 전달합니다.
       이 때, 사용자 이름값은 AWS로 명시하고, 인증하려는 Amazon ECR Registry URI를 지정합니다.
  - $ cd ~/sampleapp/amazon-eks-flask => Docker Image build 작업을 위해 위에서 Download한 Directory로 접근합니다.
  - $ docker build -t demo-flask-backend . => Image를 Build합니다.
  - $ docker images => Build한 image확인
  - $ docker tag demo-flask-backend:latest <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
     => docker tag Command를 통해, 해당 Image가 특정 Repository에 Push될 수 있도록 설정
  - $ docker push <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
     => 생성된 Image를 Push
  - ECR Management Console에 접근하여 Push된 Image가 잘 올라갔는지 확인합니다.

8. 첫 번째 Backend배포하기
  - $ mkdir -p ~/manifest/demo-flask-backend => Deployment, Apply관련 File 저장 위치 생성
  - $ cd ~/manifest/demo-flask-backend
  - $ cat <<EOF> flask-deployment.yaml => deyployment manifest생성
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-flask-backend
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-flask-backend
  template:
    metadata:
      labels:
        app: demo-flask-backend
    spec:
      containers:
        - name: demo-flask-backend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
EOF
  - $ cat <<EOF> flask-service.yaml => service manifest생성
---
apiVersion: v1
kind: Service
metadata:
  name: demo-flask-backend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/contents/aws"
spec:
  selector:
    app: demo-flask-backend
  type: NodePort
  ports:
    - port: 8080 # 서비스가 생성할 포트  
      targetPort: 8080 # 서비스가 접근할 pod의 포트
      protocol: TCP
EOF
  - $ cat <<EOF> ingress.yaml => ingress manifest생성
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: "backend-ingress"
    namespace: default
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
spec:
    rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
EOF
  - $ kubectl apply -f flask-deployment.yaml => 위에서 생성한 File들을 순차적으로 실행합니다.
  - $ kubectl apply -f flask-service.yaml
  - $ kubectl apply -f ingress.yaml => ingress object의 경우 배포시간이 걸리기에, Console창에서 
                                      Active상태를 확인합니다.
  - $ echo http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')/contents/aws
     위의 Command의 수행 결과를 Web Browser 및 API Platform에 붙여넣어 확인합니다.

8. 두 번째 Backend배포하기
  - $ mkdir -p ~/manifest/demo-nodejs-backend => Deployment, Apply관련 File 저장 위치 생성
  - $ cd ~/manifest/demo-nodejs-backend
  - $ cat <<EOF> nodejs-deployment.yaml => deyployment manifest생성
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-nodejs-backend
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-nodejs-backend
  template:
    metadata:
      labels:
        app: demo-nodejs-backend
    spec:
      containers:
        - name: demo-nodejs-backend
          image: public.ecr.aws/y7c9e1d2/joozero-repo:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
EOF
  - $ cat <<EOF> nodejs-service.yaml => service manifest생성
---
apiVersion: v1
kind: Service
metadata:
  name: demo-nodejs-backend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/services/all"
spec:
  selector:
    app: demo-nodejs-backend
  type: NodePort
  ports:
    - port: 8080
      targetPort: 3000
      protocol: TCP
EOF
  - cd ~/manifest/demo-flask-backend
  - $ cat <<EOF> ingress.yaml => ingress File의 경우 첫 번째 Backend ingress file의 경로를 추가하는 식으로 생성을 합니다.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "backend-ingress"
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
  - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
          - path: /services
            pathType: Prefix
            backend:
              service:
                name: "demo-nodejs-backend"
                port:
                  number: 8080
EOF
  - $ cd ~/manifest/demo-nodejs-backend
  - $ kubectl apply -f nodejs-deployment.yaml => 위에서 생성한 File들을 순차적으로 실행합니다.
  - $ kubectl apply -f nodejs-service.yaml
  - $ kubectl apply -f ~/manifest/demo-flask-backend/ingress.yaml => ingress object의 경우 배포시간이 걸리기에, Console창에서 
                                                                    Active상태를 확인합니다.
  - $ echo http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')/services/all
     위의 Command의 수행 결과를 Web Browser 및 API Platform에 붙여넣어 확인합니다.

9. Frontend 배포하기
  - $ mkdir -p ~/sampleapp/demo-frontend => Frontend관련  Containerize할 source를 받아 올 Directory를 생성합니다.
  - $ cd ~/sampleapp/demo-frontend
  - $ git clone https://github.com/joozero/amazon-eks-frontend.git => source를 받아옵니다.
  - $ aws ecr create-repository \ => Build한 Image를 Push할 ECR Repository를 생성합니다.
--repository-name demo-frontend \
--image-scanning-configuration scanOnPush=true \
--region ap-northeast-2
  - ECR Management console에서 위의 사항을 확인합니다. 
  - $ echo http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')/contents/'${search}'
   => 위의 명령어를 입력하여, 도출되는 URL을 ~/sampleapp/frontend/amazon-eks-frontend/src/App.js file의 url값에 입력합니다.
  - $ echo http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')/services/all
   => 위의 명령어를 입력하여, 도출되는 URL을 ~/sampleapp/frontend/amazon-eks-frontend/src/page/UpperPage.js file의 url값에 입력합니다.
  - $ cd ~
  - $ curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash => npm을 install하기 위한 Repository 추가
  - $ sudo yum install -y nodejs
  - $ node --version
  - $ npm --version
  - $ cd ~/sampleapp/frontend/amazon-eks-frontend
  - $ npm install
  - $ npm run build
  - $ docker build -t demo-frontend . => docker image생성
  - $ docker tag demo-frontend:latest <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
      => 위에서 생성한 ECR demo-frontend Repository Tagging
  - $ docker push <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
  - $ mkdir -p ~/manifest/demo-frontend => Deployment, Apply관련 File 저장 위치 생성
  - $ cd ~/manifest/demo-frontend
  - $ cat <<EOF> frontend-deployment.yaml => deyployment manifest생성
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-frontend
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-frontend
  template:
    metadata:
      labels:
        app: demo-frontend
    spec:
      containers:
        - name: demo-frontend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 80
EOF
  - $ cat <<EOF> frontend-service.yaml => service manifest생성
---
apiVersion: v1
kind: Service
metadata:
  name: demo-frontend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/"
spec:
  selector:
    app: demo-frontend
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
EOF
  - cd ~/manifest/demo-flask-backend
  - $ cat <<EOF> ingress.yaml => ingress File의 경우 첫 번째 Backend ingress file의 경로를 추가하는 식으로 생성을 합니다.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "backend-ingress"
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
          - path: /services
            pathType: Prefix
            backend:  
              service:
                name: "demo-nodejs-backend"
                port:
                  number: 8080
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "demo-frontend"
                port:
                  number: 80
EOF
  - $ cd ~/manifest/demo-frontend
  - $ kubectl apply -f frontend-deployment.yaml => 위에서 생성한 File들을 순차적으로 실행합니다.
  - $ kubectl apply -f frontend-service.yaml
  - $ kubectl apply -f ~/manifest/demo-flask-backend/ingress.yaml => ingress object의 경우 배포시간이 걸리기에, Console창에서 
                                                                    Active상태를 확인합니다.
  - $ echo http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')
    => 해당 URL을 접속하여 화면이 나타난다면 성공입니다.

10. Autoscaling Pod & Cluster
  *** kubernetes에는 크게 두 가지의 Autoscaling 기능이 있습니다. 
      - HPA(Horizontal Pod Autoscaler)
      - Cluster Autoscaler
      HPA는 CPU사용량 또는 사용자 정의 Metric을 관찰하여 Pod 개수를 자동적으로 Scale합니다. 그러나 
      해당 Pod가 올라가는 EKS Cluster 자체 자원이 모자라게 되는 경우, Cluster Autoscaler를 고려
      해야합니다.

11. HPA 적용
  - HPA Controller는 Metric값에 따라 Pod의 개수를 할당합니다. Pod Scaling을 적용하기 위해 Container에 필요한
   Resource양을 명시하고, HPA를 통해 스케일할 조건을 작성해야합니다.
  - $ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 
    => kubernetes metrics server를 생성합니다.
  - $ kubectl get deployment metrics-server -n kube-system => Metrics Server가 생성되었는지 확인합니다.
  - $ cd ~/manifest/demo-flask-backend
  - $ cat <<EOF> flask-deployment.yaml => 해당작업을 통해 Replica를 1로 설정하고 Container에 필요한 Resource양을 설정합니다.
cat <<EOF> flask-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-flask-backend
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-flask-backend
  template:
    metadata:
      labels:
        app: demo-flask-backend
    spec:
      containers:
        - name: demo-flask-backend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
EOF

cat <<EOF> nodejs-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-nodejs-backend
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-nodejs-backend
  template:
    metadata:
      labels:
        app: demo-nodejs-backend
    spec:
      containers:
        - name: demo-nodejs-backend
          image: public.ecr.aws/y7c9e1d2/joozero-repo:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
EOF

$ cat <<EOF> frontend-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-frontend
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: demo-frontend
  template:
    metadata:
      labels:
        app: demo-frontend
    spec:
      containers:
        - name: demo-frontend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 80
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
EOF
  *** 1vCPU = 1000m(milicore)
  - $ kubectl apply -f flask-deployment.yaml => 변경사항을 적용합니다.
  - $ kubectl apply -f nodejs-deployment.yaml => 변경사항을 적용합니다.
  - $ kubectl apply -f frontend-deployment.yaml => 변경사항을 적용합니다.
  - $ cat <<EOF> flask-hpa.yaml => HPA설정을 위해 아래의 yaml File도 생성합니다.
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: demo-flask-backend-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-flask-backend
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 30
EOF

$ cat <<EOF> nodejs-hpa.yaml
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: demo-nodejs-backend-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-nodejs-backend
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 30
EOF

$ cat <<EOF> frontend-hpa.yaml
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: demo-frontend-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-frontend
  minReplicas: 1
  maxReplicas: 5
  targetCPUUtilizationPercentage: 30
EOF

  - $ kubectl apply -f flask-hpa.yaml
  - $ kubectl apply -f nodejs-hpa.yaml
  - $ kubectl apply -f frontend-hpa.yaml
  - $ kubectl get hpa => 현재 hpa 상 pod갯수를 확인합니다.
  - sudo yum -y install httpd-tools => ab설치를 위해 httpd-tools를 설치합니다.
  - ab -c 200 -n 200 -t 30 http://$(kubectl get ingress/backend-ingress -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')/contents/aws
    => 위의 명령어를 통해 부하테스트를 시작합니다.
  - $ kubectl get hpa => 늘어난 hpa 상 pod갯수를 확인합니다.    

12. Cluster Autoscaler 적용
  - Traffic에 따라 Pod가 올라가는 Worker Node 자원이 모자라게 되는 경우도 발생하게 됩니다. 즉, Worker Node가
   가득차서 Pod가 Schedule될 수 없는 상태가 되는 것이죠. 이때, 사용하는 것이 Cluster Autoscaler(CA)입니다.
  - 기본적으로 설정된 Trigger의 경우 Cluster에 포함된 모든 Pod들을 10초마다 Check하며, 해당 Pod들 중 더이상 생성될 수 없는 
   Pod를 감지할 시 자동으로 확장되는 구조입니다. 그리고 해당 값은 조정이 불가능 합니다.(기본적인 Kubernetes CA에서만)
  - AWS Management Console을 통하여 Auto Scaling Group의 Max값을 5로 수정합니다.
  - $ mkdir -p ~/sampleapp/demo-catest => test용 app을 Download할 Directory를 생성합니다.
  - $ cd ~/sampleapp/demo-catest
  - $ wget https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml
  - Download한 yaml File을 열고 Cluster 이름을 설정한 후 배포합니다. 정확히는 아래 문장의 cluster-name부분을 편집합니다.
     - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<cluseter-name>
  - $ kubectl apply -f cluster-autoscaler-autodiscover.yaml
  - $ kubectl get nodes => Test를 진행하기 전 현재 Node 수 확인
  - $ kubectl create deployment autoscaler-demo --image=nginx
  - $ kubectl scale deployment autoscaler-demo --replicas=100 => Test용 Pod 생성
  - $ kubectl get deployment autoscaler-demo --watch => Pod의 배포 진행상태 확인
  - $ kubectl get nodes -w => Test를 진행중 실시간 Node 수 확인
  - $ kubectl delete deployment autoscaler-demo => Test 종료

13. Storage - EFS - Storage Class
  *** Fargate에서는 CSI Driver가 지원되지 않습니다.
  *** kubernetes Version 1.11이전에 생성한 Amazon EKS Cluster는 Storage Class와 함께 생성되지 않기에
      Cluster가 사용할 Storage Class를 정의해야합니다.
  - Storage Class는 관리자가 제공하는 Storage의 "classes"를 설명할 수 있는 방법들을 제공한다. 
  - Storage Class의 Resource
    - 각 Storage Class에는 해당 Storage Class에 속하는 persistent Volume을 동적으로 Provisioning 할 때
     사용되는 provisioner, parameters, reclaimPolicy Filed가 포함된다.
    - Storage Class에 의해 동적으로 생성된 Persistent Volume은 Class의 reclaimPolicy Field에 지정된
     Reclaim 정책을 가지는데, 이는 Delete 또는 Retain이 될 수 있다. Storage Class Object가 생성될 때
     reclaimPolicy가 지정되지 않으면 기본값은 Delete이다.
  - $ kubectl get storageclass => Cluster에 이미 있는 Storage Class를 확인합니다.
  - $ mkdir -p ~/manifest/efs-csi-controller/
  - $ cd ~/manifest/efs-csi-controller/
  - $ cat <<EOF> gp3-storage-class.yaml => gp3를 default Storage Class로 설정하기 위해 Manifest File을 생성합니다.
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  fsType: ext4 
EOF
  - $ kubectl create -f gp3-storage-class.yaml => Storage Class 생성
  - $ kubectl get storageclass => 생성 확인 
  - $ kkubectl patch storageclass <storageclass Name> -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}'
      => storageclass Name에 gp2를 입력하여, 기본값 설정을 해제한다.
  - $ kubectl get storageclass => gp2의 기본값 설정 해제 확인

14. Storage - EFS - Setting
  - 고려사항 및 사전 조건
    - Amazon EFS CSI Driver는 Windows기반 Container Image와 호환되지 않습니다.
    - Fargate Node에는 동적 영구 Volume Provisioning을 사용할 수 없지만, 정적 Provisioning은 사용할 수 있습니다.
    - 동적 Provisioning에는 1.2이상의 Driver가 필요하고, 1.17 Version이상의 Cluster가 필요합니다.
    - OIDC Provider가 필요합니다.
    - AWS CLI Version 2.4.9이상 또는 1.22.30이상이 설치되어야 합니다.
    *** AWS CLI Version Update
        - $ aws --version => 현재 Version확인
        - $ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" => Package Download
        - $ unzip awscliv2.zip => Package압축 해제
        - $ sudo ./aws/install => Package 설치
        - $ sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update => 경로 Update
        - $ aws --version => 설치 확인
        - $ aws configure list => 이전설정 확인
  - $ curl -o iam-policy-example.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/v1.3.2/docs/iam-policy-example.json
   => efs service account를 위한 정책 Download
  - $ aws iam create-policy \ => 정책 생성
--policy-name AmazonEKS_EFS_CSI_Driver_Policy \
--policy-document file://iam-policy-example.json
  - $ eksctl create iamserviceaccount \ => Service Account 생성
--name efs-csi-controller-sa \
--namespace kube-system \
--cluster <cluster-name> \
--attach-policy-arn arn:aws:iam::<Account Number>:policy/AmazonEKS_EFS_CSI_Driver_Policy \
--approve \
--override-existing-serviceaccounts \
--region <region-code>
  - $ kubectl kustomize \
"github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.3" > public-ecr-driver.yaml
     => Manifest File을 Download합니다.
  - Download받은 public-ecr-driver.yaml에서 아래 내용을 삭제합니다.
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      labels:
        app.kubernetes.io/name: aws-efs-csi-driver
      name: efs-csi-controller-sa
      namespace: kube-system
    ---
  - $ mkdir ~/manifest/efs-csi-controller
  - $ mv public-ecr-driver.yaml ~/manifest/efs-csi-controller/
  - $ kubectl apply -f ~/manifet/efs-csi-controller/public-ecr-driver.yaml => Manifest를 적용합니다.
  - $ kubectl get deployment -n kube-system efs-csi-controller => Controller실행 확인 명령어
  - $ kubectl get sa efs-csi-controller-sa -n kube-system -o yaml => Service Account 적용확인
  - $ kubectl get po -n kube-system => efs-csi-controller Addon pods 확인
  *** Amazon EFS CSI Driver는 여러 Pod간 File System을 더 쉽게 공유할 수 있도록 하는 Amazon EFS
     File System에 대한 Application별 진입점임 Amazon EFS Access Point를 지원합니다.
  - $ vpc_id=$(aws eks describe-cluster \ => Cluster가 있는 VPC ID를 검색하여 이후 단계에서 사용할 수 있도록 변수에 저장합니다.
--name <cluster name> \
--query "cluster.resourcesVpcConfig.vpcId" \
--output text)
  - $ cidr_range=$(aws ec2 describe-vpcs \ => Cluster의 VPC에 대한 CIDR범위를 검색하여 이후단계에서 사용할 수 있도록 변수에 저장합니다.
--vpc-ids $vpc_id \
--query "Vpcs[].CidrBlock" \
--output text)
  - $ security_group_id=$(aws ec2 create-security-group \ => Amazon EFS 탑재 지점에 대한 Inbound NFS Traffic을 허용하는 규칙을 사용하여 보안 그룹을 생성함과 동시에 변수에 저장합니다.
--group-name ekstest-sg-efs \
--description "Security group for EFS" \
--vpc-id $vpc_id \
--output text)
  - $ aws ec2 authorize-security-group-ingress \ => Cluster의 VPC에 대한 CIDR에서 Inbound NFS Traffic을 허용하는 Inbound규칙 생성
--group-id $security_group_id \
--protocol tcp \
--port 2049 \
--cidr $cidr_range
  - $ file_system_id=$(aws efs create-file-system \ => Amazon EFS Filesystem을 생성함과 동시에 변수에 저장합니다.
--region <region-code> \
--performance-mode generalPurpose \
--query 'FileSystemID' \
--output text)
  - $ kubectl get nodes => cluster node의 IP주소를 확인합니다.
  - $ aws ec2 describe-subnets \ => Cluster Node가 속한 Subnet을 확인합니다.
--filters "Name=vpc-id,Values=$vpc_id" \
--query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
--output table
  - $ aws efs create-mount-target \ => DNS name으로 사용할 Mount Target 생성(Node가 속한 Subnet만큼을 추가합니다.)
--file-system-id <File System ID> \
--subnet-id <subnet ID> \ => 앞서 생성한 EKS Cluster Node ID
--security-groups <Security Group ID> => 앞서 생성한 EFS Security Group ID
  - $ aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text => EFS File system ID 검색
  - $ cd ~/manifest/efs-csi-controller
  - $ curl -o efs-storage-class.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml
     => EFS의 StorageClass Manifest를 Download합니다.
  - Download후 efs-storage-class.yaml File안에 fileSystemId: fs-582a03f3 부분에 위에서 생성한 Filesystem ID로 값을 바꿉니다.
  - $ kubectl apply -f efs-storage-class.yaml => Storage Class를 배포합니다.
  - $ kubectl get storageclass => 생성 확인
  - $ cat <<EOF> flask-pvc.yaml => PVC Manifest 생성
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: flask-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 1Gi
EOF
- $ cat <<EOF> nodejs-pvc.yaml => PVC Manifest 생성
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nodejs-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 1Gi
EOF
- $ cat <<EOF> frontend-pvc.yaml => PVC Manifest 생성
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: frontend-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc
  resources:
    requests:
      storage: 1Gi
EOF
  - $ kubectl apply -f flask-pvc.yaml => pvc 생성
  - $ kubectl apply -f nodejs-pvc.yaml
  - $ kubectl apply -f frontend-pvc.yaml
  - $ kubectl get pvc => PVC 생성 확인
  - 그리고 EFS Management Console에서 Accesspoint가 생성된 걸 확인할 수 있습니다.
- $ cat <<EOF> flask-deployment.yaml => Deployment Manifest File에 PersistentVolume관련 내용 추가
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-flask-backend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-flask-backend
  template:
    metadata:
      labels:
        app: demo-flask-backend
    spec:
      containers:
        - name: demo-flask-backend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
          imagePullPolicy: Always
          volumeMounts:
            - name: flask-backend-storage
              mountPath: /var/log
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
      volumes:
        - name: flask-backend-storage
          persistentVolumeClaim:
            claimName: flask-pvc
EOF

$ cat <<EOF> nodejs-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-nodejs-backend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-nodejs-backend
  template:
    metadata:
      labels:
        app: demo-nodejs-backend
    spec:
      containers:
        - name: demo-nodejs-backend
          image: public.ecr.aws/y7c9e1d2/joozero-repo:latest
          imagePullPolicy: Always
          volumeMounts:
            - name: nodejs-backend-storage
              mountPath: /var/log
          ports:
            - containerPort: 3000
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
      volumes:
        - name: nodejs-backend-storage
          persistentVolumeClaim:
            claimName: nodejs-pvc
EOF

$ cat <<EOF> frontend-deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-frontend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-frontend
  template:
    metadata:
      labels:
        app: demo-frontend
    spec:
      containers:
        - name: demo-frontend
          image: <Account Number>.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
          imagePullPolicy: Always
          volumeMounts:
            - name: frontend-storage
              mountPath: /var/log/nginx
          ports:
            - containerPort: 80
          resources:
            requests:
              cpu: 250m
            limits:
              cpu: 500m
      volumes:
        - name: frontend-storage
          persistentVolumeClaim:
            claimName: frontend-pvc
EOF
  - $ kubectl apply -f flask-deployment.yaml => 수정사항을 적용합니다.
  - $ kubectl apply -f nodejs-deployment.yaml  
  - $ kubectl apply -f frontend-deployment.yaml
  - Pod의 숫자 조절을 위해 각 Service당 hpa File의 min값을 2로 조정합니다.
  - 위의 사항들을 적용하기 위해 기존의 pod를 삭제하여 강제로 deployment를 다시 생성합니다.
  - $ kubectl get po => 현재 생성된 Pod들을 확인합니다.
  - $ kubectl exec --stdin --tty <Pod Name> -- /bin/sh => 생성된 Pod의 Shell로 진입합니다.
  - # cd /var/log => 공유된 Directory로 진입합니다.
  - # touch test1 => 공유현황을 Test하기 위해 File을 생성합니다.
  - # exit => Pod를 나갑니다.
  - $ kubectl exec --stdin --tty <Pod Name> -- /bin/sh => 앞서 접속한 Pod와 다른 Shell로 진입합니다.
  - # ls -al /var/log => 위에서 해당 Directory에 생성한 test1 File을 확인합니다.

15. HTTPS Setting
  - ACM(AWS Certificate Manager)
   - AWS Certificate Manager(ACM) => 이 Service는 TLS를 사용하는 보안 Web이 필요한 기업 고객을 위해 인증서를 관리합니다.
   - ACM Private CA => 이 Service는 AWS Cloud 내부에 PKI(Public Key Infra)를 구축하는 기업고객을 대상으로 하며, 조직 내
                      에서 비공개로 사용할 수 있도록 고안되었습니다. ACM Private CA를 사용하여 고유한 CA(인증 기관) 계층을
                      만들고 사용자, Computer, Application, Service, Server 및 기타 Device를 인증하기 위해 인증서를 발급
                      할 수 있습니다. 그리고 사설 CA에서 발급한 인증서는 Internet에서 사용할 수 없습니다.
   - 요금 => ACM의 경우 요금을 지불하지 않습니다. 다만 ACM Private CA의 경우 요금이 부과됩니다.
  - 사전 준비
   - Route53에 등록된 Domain 및 현재 Ingress ALB의 DNS Name과 Mapping된 A Record가 있어야합니다.
   - 해당 Domain을 기준으로 발급된 ACM이 필요합니다.
   - 앞서 frontend 생성 시 수정했던 URL을 https://<Domain Name>/으로 수정합니다.
     - https://<Domain Name>/contents/'${search} URL을 ~/sampleapp/frontend/amazon-eks-frontend/src/App.js file의 url값에 입력합니다.
     - https://<Domain Name>/services/all URL을 ~/sampleapp/frontend/amazon-eks-frontend/src/page/UpperPage.js file의 url값에 입력합니다.
   - 위의 사항을 Frontend Service에 적용합니다.
     - $ cd ~/sampleapp/frontend/amazon-eks-frontend
     - $ npm install
     - $ npm run build
     - $ docker build -t demo-frontend .
     - $ docker push 642057397966.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
     - ECR Management Console에서 새로운 Image가 Push되었는지 확인합니다.
     - $ kubectl delete deployment demo-frontend => 현재 운영 중인 Deployment 및 Service를 종료한 후 다시 실행시켜줍니다.
     - $ kubectl delete service demo-frontend 
     - $ cd ~/manifest/demo-frontend/
     - $ kubectl apply -f frontend-deployment.yaml
     - $ kubectl apply -f frontend-service.yaml
  - $ cat <<EOF> ingress.yaml => #HTTPS Settings 아래의 세 줄 및 각 Path별 ssl-redirect설정을 추가합니다.
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "backend-ingress"
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
    #HTTPS Settings
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/certificate-arn: <ACM Arn> #ACM의 ARN을 입력합니다.
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
spec:
  rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: ssl-redirect
                port:
                  name: use-annotation
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
          - path: /services
            pathType: Prefix
            backend:
              service:
                name: ssl-redirect
                port:
                  name: use-annotation
          - path: /services
            pathType: Prefix
            backend:
              service:
                name: "demo-nodejs-backend"
                port:
                  number: 8080
          - path: /
            pathType: Prefix
            backend:
              service:
                name: ssl-redirect
                port:
                  name: use-annotation
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "demo-frontend"
                port:
                  number: 80
EOF
     - $ kubectl apply -f ingress.yaml
     - 이 후 페이지 접속 및 AWS Management Console에서 ALB관련 설정사항들의 적용을 확인합니다.