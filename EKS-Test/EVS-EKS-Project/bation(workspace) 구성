### EKS 쿠버네티스와 연결하여 노드와 파드를 생성 및 관리할 서버에서의 스크립트

1. kubectl 설치
curl -o kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.21.2/2021-07-05/bin/linux/amd64/kubectl 
# 1.21버전 쿠버네티스 curl

chmod +x ./kubectl
# 사용할 환경병수 경로로 등록하거나 /usr/local/bin의 경로로 이동
# mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin

kubectl version --short --client # 버전확인

echo 'source <(kubectl completion bash)' >>~/.bashrc # kubectl 자동완성

2. ekstl 설치
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /$HOME
# ekstl 설치

chmod +x ./eksctl
# 사용할 환경병수 경로로 등록하거나 /usr/local/bin의 경로로 이동
# mkdir -p $HOME/bin && cp ./kubectl $HOME/bin/kubectl && export PATH=$PATH:$HOME/bin

eksctl version # 버전확인

AWS CLI Version Update 2버전을 사용하거나 1.23버전 이상을 사용할 수 있다.
aws --version # 현재 Version확인
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" # Package Download
unzip awscliv2.zip # Package 압축 해제
sudo ./aws/install # Package 설치
sudo ./aws/install --bin-dir /usr/local/bin --install-dir /usr/local/aws-cli --update # 경로 설정
aws --version # 버전 확인
aws configure list # confiure 확인

configure list가 없으면 추가로 aws configure에 IAM의 액세스키와 프라이빗키를 등록한다

3. eks 클러스터 & 노드그룹 구축 
# 클러스터 역할과 NodeGroup 역할은 검색해서 만들면 된다. 굉장히 잘 나와있으나 왜 필요한가를 고민해야 한다.
# 하지만, 기본적으로 default로 만들면 모든 정책이 포함된 IAM 역할이 생성된다. 비교하면서 사용하여야 한다.
# 첫번째와 두번째를 생성하며, 비교하여 확실히 정리할 수 있도록 한다. 이 IAM만 이해해도 뒤에 나오는 애드온으로 생성되는 pod들을 이해할 수 있다.
# 정확히 이해했는지 나중에 꼭 확인하겠다.

ekstl을 사용하여 eks 클러스터와 노드그룹을 생성한다.

eks-cluster-create.yaml을 만든다.

아래 manifast를 eksctl create cluster -f eks-cluster-create.yaml명령어로 cluster&nodegroup을 생성한다
=======================================================================
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
 
metadata:
  name: Fin-EKS
  region: ap-northeast-2
  version: "1.21"

vpc:
  subnets:
    private:
      ap-northeast-2a: { id: subnet-0a611abd167e1afd8 }
      ap-northeast-2c: { id: subnet-01c5cf55f8bf57101 }
iam: 
  serviceRoleARN: "arn:aws:iam::399319344088:role/EKS-Cluster-Role"      

managedNodeGroups:
  - name: Fin-EKS-Worker
    instanceType: t3.medium
    instanceName: Fin-EKS-Worker
    privateNetworking: true
    # autoscaling
    minSize: 2
    maxSize: 3
    desiredCapacity: 2
    # volume
    volumeType: gp3
    volumeSize: 20
    # ssh 
    ssh:
      allow: true
      # keypair-pem key
      publicKeyName: TESTkey
      # new feature for restricting SSH access to certain AWS security group IDs
      sourceSecurityGroupIds: ["sg-0994e0fc787af0b8b"] # eks 노드구룹 ssh 용 보안그룹
    labels: {role: worker}	
    tags:
      nodegroup-role: worker
    iam:
      instanceRoleARN: "arn:aws:iam::399319344088:role/EKS-Nodegroup-Role"
================================================================
클러스터가 생성되면 kubeconfig를 생성해야 한다.
aws eks update-kubeconfig --region region-code --name my-cluster

확인 명령어 kubectl get svc
##NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
##kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   11m

클러스터가 생성되었고, workspace에서 관리할 수 있는 환경이 완성 되었다.
cloud formation으로 구성되는 cluster&nodegroup의 구성 리소스와 config를 확인하여
정리할 수 있어야 한다.

노드 그룹 확인 명령어
kubectl get nodes -A
##NAME                                                STATUS   ROLES    AGE     VERSION
##ip-192-168-11-221.ap-northeast-2.compute.internal   Ready    <none>   2m14s   v1.21.5-eks-9017834
##ip-192-168-12-43.ap-northeast-2.compute.internal    Ready    <none>   2m13s   v1.21.5-eks-9017834

pod 확인 명령어
kubectl get pod -A


4. docker 인스톨
# 쿠버네티스의 pod들은 도커 이미지 위에서 돌아가는 어플리케이션들이다.
# 원할한 테스트를 위해서 docker를 사용한 이미지 빌드, 배포 등을 진행하여야 한다.

sudo amazon-linux-extras install -y docker # 아마존 리눅스용 도커 설치

sudo service docker start # 도커 start

sudo usermod -a -G docker ec2-user # 도커 그룹에 ec2-user 등록(세션 종료 후 적용 됨)

docker info # 도커 설치 및 권한 획득 확인

ECR 생성 및 로그인 인증 # 도커 이미지를 푸쉬하고 풀링하기 위한 저장소 생성

aws ecr create-repository \      # Repository생성 
--repository-name demo-flask-backend \
--image-scanning-configuration scanOnPush=true \
--region ap-northeast-2

aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin 'Account Number'.dkr.ecr.ap-northeast-2.amazonaws.com # 어카운트 넘버를 사용하여 ECR에 토큰 인증을 통한 도커 로그인을 진행하여 ecr과 연결

git clone https://github.com/joozero/amazon-eks-flask.git # git으로 aws 샘플예저 다운

git으로 설치한 Dokerfile로 이미지 빌드

docker build -t demo-flask-backend . # 마지막에 .은 현재 경로를 의미

docker images # Dockerfile로 만들어진 demo-flask-backend를 확인

docker tag demo-flask-backend:latest 'Account Number'.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest 
# 도커파일에 ecr 태그
docker push 'Account Number'.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest # ecr에 도커파일 푸쉬

docker rmi 'imagesid' #도커 이미지 삭제

지금까지 workspace에 설정하거나 쿠버네티스의 기본이 되는 사항은 확인하였고, 실제로 쿠버네티스로 서비스를 배포하여 진행하겠다.

5. ALB Ingress 구성
# pod를 프라이빗 ip로 구성하였기 때문에, 기본적으로 외부에서 접근이 힘들다. 이를 위해 우리는 로드밸런서를 사용할 수 있다.
# 쿠버네티스를 통해 AWS ALB를 생성하여 파드들을 타겟으로 하는 INngress 구성을 할 수 있다.

Amazon EKS는 기본으로 Ingress Controller를 ALB로 사용할 수 있다.
OIDC를 통한 인증으로 쿠버네스가 AWS 리소스인 ALB를 생성하기 위해 쿠버네티스(Service Account)와 AM OIDC Provider 연결을 한다.
OIDC로 인증을 진행하면, 쿠버네티스가 eks 클러스터의 oidc를 통해서(쿠버->AWS) 서비스어카운트를 생성하며 AWS IAM ROLE과 연동할 수 있다.

oidc 인증
eksctl utils associate-iam-oidc-provider --region $AWS_REGION --cluster $CLUSTER_NAME --approve
# oidc에 인증 받을 정보를 입력

인증 확인
aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.identity.oidc.issuer" --output text
##출력
##https://oidc.eks.ap-northeast-2.amazonaws.com/id/285D3362F5DF0B659F9D796A8FA84A87

Ingress에서 사용될 alb Controller에 부여할 IAM Policy생성
aws iam create-policy \
--policy-name AWSLoadBalancerControllerIAMPolicy \
--policy-document https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.4.1/docs/install/iam_policy.json
# 마지막 줄 json 파일을 열어서 워크스페이스에 json파일 ex) alb_contorller_policy.json 만들어서하면 기록을 남길 수 있다.

위에서 생성한 Policy를 연결하며 서비스어카운트를 생성한다.
eksctl create iamserviceaccount \
--cluster $CLUSTER_NAME \
--name aws-load-balancer-controller \
--namespace kube-system \
--attach-policy-arn arn:aws:iam::399319344088:policy/ALBIngressControllerIAMPolicy \
--override-existing-serviceaccounts \
--approve

serviceaccount가 생성된 걸 확인할 수 있다.
##[ec2-user@ip-192-168-101-95 policy]$ kubectl get serviceaccounts -A | grep alb
##kube-system       aws-load-balancer-controller               1         18m


AWS Load Balancer Controller를 설치
Cert Manaber를 설치한다. # https인증서를 위한 CM 설치

kubectl apply \
--validate=false \
-f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.yaml
# 안에 내용을 봐도 모르니 그냥 설치하는게 마음이 편하다

AWS Load Balancer Controller를 설치
wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.4.1/v2_4_1_full.yaml
# 2.4.1버전의 컨트롤러 설치

sed -i.bak -e 's|your-cluster-name|my-cluster|' ./v2_4_1_full.yaml # 클러스터 이름을 바꾼다

그 후 편집기를 열어서 serviceaccount 부분을 제거한다. 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/name: aws-load-balancer-controller
  name: aws-load-balancer-controller
  namespace: kube-system
---
# 앞에서 이미 서비스어카운트를 만들었기 때문에 다시 만들면 앞에껄 덮어쓴다.

kubectl apply -k "github.com/aws/eks-charts/stable/aws-load-balancer-controller/crds?ref=master"
# 컨트롤러 배포 전 yaml파일 오류로 파라미터를 수정해야 한다.
kubectl apply -f v2_4_1_full.yaml
# 컨트롤러 배포

결과 확인 한다.
kubectl get deployment -n kube-system aws-load-balancer-controller

테스트용 샘플 예제
---
# 1. Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx
spec:
  selector:
    matchLabels:
      run: my-nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80

# 2. Service
---
apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
  type: NodePort
  selector:
    run: my-nginx

# 3. Ingress
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "ingress-app"
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/subnets: subnet-0c9cf0d84837c68c7, 	subnet-0a77b7fd0040df848 #이걸 사용하면 정책 추가 
    # kubectl logs -n kube-system $(kubectl get po -n kube-system | egrep -o "aws-load-balancer[a-zA-Z0-9-]+") alb 컨트롤러 로그 확인
                                                 
spec:
  rules:
  - http:
    paths:
      - path: /
          pathType: Prefix
          backend:
          service:
            Name: "my-nginx"
            Port:
              number: 80
---
샘플예제 중 deployment, service는 묶어서 한번에 디플로이하고, ingress 는 따로 하는게 관리하기 편함
kubectl get ingresses.networking.k8s.io 명령어를 치면 alb목록이 나옴

ingress삭제 생성할 때 사용한 yaml파일을 kubectl delete -f ingress.yaml로 삭제해도 되지만
이미 생성되면 그때부터 ingress는 오브젝트로 생성되기 때문에

kubectl delete ingresses.networking.k8s.io 'ingress-name'으로 삭제 가능함

6. 서비스 배포
# 샘플예제를 사용해도 되지만 다른 기능도 확인을 위한 서비스를 도커로 만들어 ecr에 푸시하고 pod로 배포한다.
# 위에서 만든 ecr과 저장된 이미지를 사용한다.

deplyment+service flask.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-flask-backend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-flask-backend
  template:
    metadata:
      labels:
        app: demo-flask-backend
    spec:
      containers:
        - name: demo-flask-backend
          image: 399319344088.dkr.ecr.ap-northeast-2.amazonaws.com/demo-flask-backend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 8080

---
apiVersion: v1
kind: Service
metadata:
  name: demo-flask-backend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/contents/aws"
spec:
  selector:
    app: demo-flask-backend
  type: NodePort
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP


ingress.yaml # alb를 공유하기 위해 밑에서도 사용할 예정
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: "backend-ingress"
    namespace: default
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
spec:
    rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080


위에 매니페스트를 apply 하면 웹서비스가 생성되고 alb를 통해서 브라우저에서 보인다

추가로 node.js 랑 동시에 위의 flask 서비스랑 alb를 사용하겠다.

node.js의 이미지는 public 이미지를 사용한다.

node.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-nodejs-backend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-nodejs-backend
  template:
    metadata:
      labels:
        app: demo-nodejs-backend
    spec:
      containers:
        - name: demo-nodejs-backend
          image: public.ecr.aws/y7c9e1d2/joozero-repo:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 3000

---
apiVersion: v1
kind: Service
metadata:
  name: demo-nodejs-backend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/services/all"
spec:
  selector:
    app: demo-nodejs-backend
  type: NodePort
  ports:
    - port: 8080
      targetPort: 3000
      protocol: TCP

위에서 만든 ingress.yaml에 노드 서비스를 추가하여 같은 alb를 사용할 수 있다.
ingress.yaml 수정
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
    name: "backend-ingress"
    namespace: default
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/target-type: ip
spec:
    rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
          - path: /services
            pathType: Prefix
            backend:
              service:
                name: "demo-nodejs-backend"
                port:
                  number: 8080
# path를 하나더 추가하여 nodejs의 서비스 name을 넣으면 된다.

apply를 하면 alb는 그대로며 타겟그룹만 추가된 걸 확인할 수 있다. 

albdns/services/all 에 새로운 페이지가 확인 된다.
#ALB를 하나만 사용하는 걸 테스트 했으니 다시 기존 flask예제에 추가하여 진행하겟다

git clone https://github.com/joozero/amazon-eks-frontend.git # fornt 파일을 받는다

amazon-eks-frontend/src/App.js file의 url에 ALBDNS/contents/${search}를 넣는다
# ex) 'http://k8s-default-backendi-884ae190fd-843783675.ap-northeast-2.elb.amazonaws.com/contents/${search}'

amazon-eks-frontend/src/page/UpperPage.js file의 url에 ALBDNS/services/all을 넣는다
# ex) 'http://k8s-default-backendi-884ae190fd-843783675.ap-northeast-2.elb.amazonaws.com/services/all'

여기서 사용한 프론트는 node.js + nginx를 사용하한다. 빌드를 로컬에서 한번 한다.
그러기 위해 npm을 설치 한다. # nodjs에 js를 사용하기 위해서 만든건데 모름.

curl -sL https://rpm.nodesource.com/setup_14.x | sudo bash
sudo yum install -y nodejs
npm install # 인스톨하고 error가 뜨면 npm audit fix 명령어 사용, 몇 번 반복하면 취약점이 줄어들고 더 이상 안줄어들면, node_modules과 package.lock.json을 삭제하고 다시 npm install하면 된다. 주위에 nodejs개발자가 있어도 모른다.
# 이 부분은 이해하려고 하면 힘들다. 난 일단 포기함 의존성 패키지 문젠데 라이브러리 만든사람이 업데이트 안 해서 그런건데 도저히 트러블 슈팅이 힘들다.
#그런데 왜 이딴 예제를 쓰냐고 물어보면 aws eks에서 나오는 sample 예제다 다른 예제 있으면 그거 쓰는게 정신건강에 좋다.
npm run build # 결론은 이걸 하기 위한거다

docker build -t demo-frontend . # 도커로 말기

aws ecr create-repository \ # ecr 생성 front
--repository-name demo-flask-frontend \
--image-scanning-configuration scanOnPush=true \
--region ap-northeast-2

docker tag demo-frontend:latest 399319344088.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest # ecr과 tag 맞추기
docker push 399319344088.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest # ecr에 푸시

프론트 deploy.yaml을 만든다. # service 포함
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-frontend
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-frontend
  template:
    metadata:
      labels:
        app: demo-frontend
    spec:
      containers:
        - name: demo-frontend
          image: 399319344088.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: demo-frontend
  annotations:
    alb.ingress.kubernetes.io/healthcheck-path: "/"
spec:
  selector:
    app: demo-frontend
  type: NodePort
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

ingress에 추가한다.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: "backend-ingress"
  namespace: default
  annotations:
    kubernetes.io/ingress.class: alb
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: ip
spec:
  rules:
    - http:
        paths:
          - path: /contents
            pathType: Prefix
            backend:
              service:
                name: "demo-flask-backend"
                port:
                  number: 8080
          - path: /services
            pathType: Prefix
            backend:
              service:
                name: "demo-nodejs-backend"
                port:
                  number: 8080
          - path: /
            pathType: Prefix
            backend:
              service:
                name: "demo-frontend"
                port:
                  number: 80

ingress.ymal까지 배포하면 샘플예제 완성

# 샘플예제는 쿠버네티스와 도커의 전반적인 내용을 가지고 있기 때문에 꼭 완료할 수 있어야 한다.


7. nodegroup as (CA)와 pod autoscaling(HPA)
pod가 추가로 생성될 때 node의 리소스를 다 사용하면 pod는 배포되지 못하고 pending 상태가 된다.
노드가 오토스케일링 그룹에 설정되어 있지만, 쿠버네티스에서 추가 설정을 하지 않으면 추가된 노드 인스턴스에는
pod가 생성되지 않는다. 그렇기에 CA(Cluster AutoScaling)을 사용하여 연동시켜야 한다.
pod를 기준으로 CA가 진행되기 때문에 먼저 pod HPA먼저 구성하겠다.

메트릭서버 배포
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
# HPA는 파드의 리소스 상태를 보고 리소스 점유율이 높아지면 새로운 파드를 생성한다
# 상태를 볼 수 있는 메트릭 서버를 배포한다.

kubectl get deployments.apps -n kube-system metrics-server # 메트릭 서버 생성 확인

테스트용 pod를 배포하기 위한 yaml을 만든다
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  replicas: 1
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache
# 레플리카셋은 1고 기존이랑 다른 리소스 리밋을 추가하였다.
# 배포를 진행한다.

HPA 구성 v2
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-php
  namespace: deafault
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
status:
  currentReplicas: 1
  desiredReplicas: 1
  currentMetrics:
  - type: Resource
    resource:
      name: cpu
      current:
        averageUtilization: 0
        averageValue: 0

hpa 구성 v1
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50

kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 # 이 명령어로 v1의 메니패스트 hpa 배포 가능

배포 후 kubectl get hpa hpa가 갈려있는 deployment들을 확인할 수 있다.

kubectl run -i \
    --tty load-generator \ #pod로 생성 됨
    --rm --image=busybox \
    --restart=Never \
    -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"
#파드 부하 테스트 

잠시후 확인하면 부하증가로 인한 파드가 오토스케일링된 걸 확인할 수 있다.
kubectl get hpa php-apache
##NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
##php-apache   Deployment/php-apache   230%/50%   1         10        6          10m55s

부하 테스트를 종료하면 파드가 줄어든다.
# 파드는 증가하는데 파드가 생성될 노드가 없으니 파드가 늘지 않고 대기 상태가 된다
# 그렇기 때문에 노드그룹을 확장할 수 있는 CA를 적요해야 한다.


CA용 yaml을 다운
curl -o cluster-autoscaler-autodiscover.yaml https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml

다운 받은 후 마지막 부분에 my-cluster 부분 수정한다.    
그리고 밑에 두 줄도 넣는다
spec:
      containers:
      - command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<YOUR CLUSTER NAME>
        - --balance-similar-node-groups        # 추가해야 함
        - --skip-nodes-with-system-pods=false  # 추가해야 함


그리고 aws리소스(오토스케일링)을 사용하니 권한을 줘야 한다.

cluster-autoscaler-policy.json을 만든다.
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": [
                "autoscaling:DescribeAutoScalingGroups",
                "autoscaling:DescribeAutoScalingInstances",
                "autoscaling:DescribeLaunchConfigurations",
                "autoscaling:DescribeScalingActivities",
                "autoscaling:DescribeTags",
                "autoscaling:SetDesiredCapacity",
                "autoscaling:TerminateInstanceInAutoScalingGroup",
                "ec2:DescribeLaunchTemplateVersions",
                "ec2:DescribeInstanceTypes"
            ],
            "Resource": "*",
            "Effect": "Allow"
        }
    ]
}

aws iam 정책으로 만든다

aws iam create-policy \
    --policy-name AmazonEKSClusterAutoscalerPolicy \
    --policy-document file://cluster-autoscaler-policy.json

eksctl을 통해 서비스어카운트를 만들고 정책과 연결한다.

eksctl create iamserviceaccount \
  --cluster=Fin-EKS2 \
  --namespace=kube-system \
  --name=cluster-autoscaler \
  --attach-policy-arn=arn:aws:iam::399319344088:policy/AmazonEKSClusterAutoscalerPolicy \
  --override-existing-serviceaccounts \
  --approve

kubectl apply -f cluster-autoscaler-autodiscover.yaml # CA를 배포한다.

배포 후 service account를 교체한다.
kubectl annotate serviceaccount cluster-autoscaler \
-n kube-system \
eks.amazonaws.com/role-arn=arn:aws:iam::399319344088:policy/AmazonEKSClusterAutoscalerPolicy
# 배포 파일에 service account를 바꿔줘야 할 것 같은데 포기

배포 파일 업데이트
##kubectl patch deployment cluster-autoscaler \
##  -n kube-system \
##  -p '{"spec":{"template":{"metadata":{"annotations":{"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"}}}}}' 

# -p옵션 때문에 주석이 막혀서 위에 주석처리 했는데. 반드시 해야함

버전도 맞춰줘야 함 https://github.com/kubernetes/autoscaler/releases 여기서 쿠버네티스 버전과 매칭
kubectl set image deployment cluster-autoscaler \
  -n kube-system \
  cluster-autoscaler=k8s.gcr.io/autoscaling/cluster-autoscaler:v<1.21.n>
# 배포파일이 너무 구려서 바꿀게 많음

kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler
# ca 로그 확인

kubectl create deployment autoscaler-demo --image=nginx # 테스트 pod 배포

kubectl scale deployment autoscaler-demo --replicas=100 # 생성한 pod 100개로 레플셋 구성

kubectl get deployment autoscaler-demo --watch # 늘어나는 pod 구경

kubectl get nodes # node 개수 증가 확인

kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler # 노드가 max 확인

kubectl delete deployment autoscaler-demo 테스트 종료

8. EFS 생성 및 연결
# 쿠버네티스에는 여러가지 영구볼륨을 지원한다. 이중 EFS를 마운트하여 어러 pod에서 공유해서 사용하는 구성을 하겠다.
# 중요한 항목으로는 PV(storage class), PVC(claim) 확실히 이해해야 함

aws에서 제공하는 스토리지인 EBS를 배포하는 매니페스트이다. # 기본적으로 ebs 볼륨이 있지만 추가해서 pv(storageclass)를 이해 한다.
                                                        # EFS,EBS 모두 쿠버네티스에서 사용할 수 있는 pv 이다.
vi ebsstorageclass.yaml 생성
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp2-test
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
  fsType: ext4 

kubectl apply -f <ebsstorageclass>.yaml
스토리지 클래스를 배포하면 프로비저닝 된 ebs 볼륨이 생성된다. # 이미 gp2는 있음

kubectl get storageclasses.storage.k8s.io -A
NAME        PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2-test    kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  130m


kubectl annotate storageclass gp2 storageclass.kubernetes.io/is-default-class=true
#생성한 스토리지클래스에 주석을 달아서 ebs(gp2)스토리지를 디폴트 스토리지로 설정한다.

우리가 사용할 스토리지는 aws의 네트워크 스토리지인 EFS이다.

ingress와 마찬가지로 쿠버네티스에서 aws 리소스를 사용하기 위해서는 ServiceAccount, driver 등이 필요하다.

먼저 efs csi driver를 배포하기 위한 ServiceAccount를 생성한다.(IAM Policy role과 연동되는)

서비스 어카운트와 롤에서 사용 될 정책인 AWS efs 관리형 IAM 정책을 다운 받는다.
curl -o iam-policy-example.json https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/v1.3.2/docs/iam-policy-example.json

iam-EFS-policy-example.json (권한 확인)
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:DescribeAccessPoints",
        "elasticfilesystem:DescribeFileSystems"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "elasticfilesystem:CreateAccessPoint"
      ],
      "Resource": "*",
      "Condition": {
        "StringLike": {
          "aws:RequestTag/efs.csi.aws.com/cluster": "true"
        }
      }
    },
    {
      "Effect": "Allow",
      "Action": "elasticfilesystem:DeleteAccessPoint",
      "Resource": "*",
      "Condition": {
        "StringEquals": {
          "aws:ResourceTag/efs.csi.aws.com/cluster": "true"
        }
      }
    }
  ]
}

위에선 만든 권한으로 정책을 만든다. # 정책은 IAM ROLE에서 사용된다.
aws iam create-policy \
    --policy-name AmazonEKS_EFS_CSI_Driver_Policy \
    --policy-document file://iam-EFS-policy-example.json

efs 컨트롤러 service account 생성
eksctl create iamserviceaccount \
    --name efs-csi-controller-sa \
    --namespace kube-system \
    --cluster 'Cluster-Name' \
    --attach-policy-arn arn:aws:iam::399319344088:policy/AmazonEKS_EFS_CSI_Driver_Policy \
    --approve \
    --override-existing-serviceaccounts \
    --region 'region-code'

kubectl get serviceaccounts -n kube-system # service account 생성 확인

EFS 컨트롤 드라이버 다운
kubectl kustomize \
    "github.com/kubernetes-sigs/aws-efs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.3" > efs_driver.yaml 
# efs 컨트롤 드라이버 매니패스트 efs_drvier.yaml을 확인

EFS 컨트롤 드라이버 매니패스트에서 제일 위 Service Account 부분을 제거 # 위에서 서비스 어카운트를 생성했기 때문에
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: aws-efs-csi-driver
  name: efs-csi-controller-sa
  namespace: kube-system
---

EFS 컨트롤러 배포
kubectl apply -f efs_driver.yaml

노드와 컨트롤러 2개가 배포된 걸 확인 + csidriver

efs-csi-controller-84ddc4dd68-22gl5            3/3     Running   0          20h
efs-csi-controller-84ddc4dd68-kt4vd            3/3     Running   0          20h
efs-csi-node-mq2mh                             3/3     Running   0          20h
efs-csi-node-nzc4s                             3/3     Running   0          20h

EFS 생성

VpcId와 VpcCidr 등을 변수 등록하여 터미널에서 작업하기 쉽게 만든다.

vpc_id=$(aws eks describe-cluster \ # echo $vpc_id
    --name <my-cluster> \
    --query "cluster.resourcesVpcConfig.vpcId" \
    --output text)

cidr_range=$(aws ec2 describe-vpcs \ # echo $cidr_range
    --vpc-ids $vpc_id \
    --query "Vpcs[].CidrBlock" \
    --output text)

변수 설정 확인
[ec2-user@ip-192-168-101-95 ~]$ echo $vpc_id
vpc-02252a824ea1657f3
[ec2-user@ip-192-168-101-95 ~]$ echo $cidr_range
192.168.0.0/16

EFS의 엑세스 포인트(연결)을 허용할 보안그룹을 생성
security_group_id=$(aws ec2 create-security-group \ # echo $sercurity_group_id
    --group-name MyEfsSecurityGroup \
    --description "My EFS security group" \
    --vpc-id $vpc_id \
    --output text)

생성한 보안그룹에 인바운드 트래픽 허용
aws ec2 authorize-security-group-ingress \
    --group-id $security_group_id \
    --protocol tcp \
    --port 2049 \
    --cidr $cidr_range

EFS 파일 시스템 생성
file_system_id=$(aws efs create-file-system \ # echo $file_system_id
    --region 'region-code' \
    --performance-mode generalPurpose \
    --query 'FileSystemId' \
    --output text)

VPC에서 사용할 수 있는 서브넷과 AZ 확인 # EFS와 연결을 위한 노드 서브넷 검색
aws ec2 describe-subnets \
    --filters "Name=vpc-id,Values=$vpc_id" \
    --query 'Subnets[*].{SubnetId: SubnetId,AvailabilityZone: AvailabilityZone,CidrBlock: CidrBlock}' \
    --output table

EFS에 노드그룹이 있는 서브넷과 연결(서브넷에 ENI 생성하여 연결)
#예제
aws efs create-mount-target \
--file-system-id $file_system_id \
--subnet-id subnet-EXAMPLEe2ba886490 \
--security-groups $security_group_id

첫번째 노드 서브넷을 마운트타겟 연결
aws efs create-mount-target \
--file-system-id $file_system_id \
--subnet-id subnet-0a611abd167e1afd8 \ 
--security-groups $security_group_id

두번째 노드 서브넷을 마운트타겟 연결
aws efs create-mount-target \
--file-system-id $file_system_id \
--subnet-id subnet-01c5cf55f8bf57101 \ 
--security-groups $security_group_id

EFS 스토리지클래스 배포할 StorageClass.yaml 매니패스트 다운
curl -o storageclass.yaml https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/master/examples/kubernetes/dynamic_provisioning/specs/storageclass.yaml

다운받은 매니패스트에서 EFS fileSystemID를 변경한다
aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text #내 EFS의 파일시스템 ID를 확인
fs-085b9e63a116ce157

storageClass = EFS 라고 생각하면 됨
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: efs-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-085b9e63a116ce157 # 이 부분 변경
  directoryPerms: "700"
  gidRangeStart: "1000" # optional
  gidRangeEnd: "2000" # optional
  basePath: "/dynamic_provisioning" # optional

스토리지클래스 배포
kubectl apply -f storageclass.yaml

PVC(efs 엑세스 포인트) 배포 # 스토리지를 사용하는 pod당 pvc는 하나씩 배포 해야한다.
pvc용 매니패스트 작성
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: efs-claim
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: efs-sc # 스토리지 클래스의 이름을 작성
  resources:
    requests:
      storage: 5Gi

스토리지를 사용하는 pod 매니패스트 예시
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 80
        volumeMounts:
          - name: test-volume
            mountPath: /data
      volumes:
        - name: test-volume
          persistentVolumeClaim:
            claimName: <PVC_NAME>

pod와 pvc 매니페스트를 사용해서 배포
kubectl get -f pvc+pod.yaml

콘솔에서 pvc(엑세스포인트) 생성 확인

kubectl get pvc # 생성된 PersistentVolumeClaim에 대한 세부 정보를 봅니다

kubectl exec efs-app -- bash -c "cat data/test.txt" # 디렉토리에서 확인

...
Tue Mar 23 14:29:16 UTC 2021
Tue Mar 23 14:29:21 UTC 2021
Tue Mar 23 14:29:26 UTC 2021
Tue Mar 23 14:29:31 UTC 2021
...

위에 storageclass 배포 부터 다른 방향으로 하는 법 # 위 방법으로 하면 공유가 안됨 예상으로는 root계정이슈 테스트 해 볼 생각
git clone https://github.com/kubernetes-sigs/aws-efs-csi-driver.git

cd aws-efs-csi-driver/examples/kubernetes/multiple_pods/
tree # tree 설치한 후

aws efs describe-file-systems --query "FileSystems[*].FileSystemId" --output text
fs-0957d5a6758fb54d1

kubectl apply -f specs/storageclass.yaml
cat specs/pv.yaml 에서 fs-아이디 변경
kubectl apply -f pv.yaml
kubectl apply -f claim.yaml

kubectl describe pv efs-pv

kubectl apply -f 'pod배포'.yaml # 두개 다 배포

그 후 테스트 하면 잘 됨

kubectl exec <pod명> --stdin --tty -- /bin/bash # 파드에 접속
cd /data # 공유 확인


지금까지 사용한 매니패스트 옵션 모음(Deployment)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache2
spec:
  selector:
    matchLabels:
      run: php-apache2
  replicas: 1
  template:
    metadata:
      labels:
        run: php-apache2
    spec:
      containers:
      - name: php-apache2
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 600m
          requests:
            cpu: 200m
        volumeMounts:
        - name: persistent-storage
          mountPath: /data
      volumes:
      - name: persistent-storage
        persistentVolumeClaim:
          claimName: efs-claim2
























